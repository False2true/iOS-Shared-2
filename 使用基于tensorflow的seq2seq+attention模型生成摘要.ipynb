{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strQ2B(ustring):\n",
    "    \"\"\"全角转半角\"\"\"\n",
    "    rstring = \"\"\n",
    "    for uchar in ustring:\n",
    "        inside_code=ord(uchar)\n",
    "        if inside_code == 12288:                              #全角空格直接转换            \n",
    "            inside_code = 32 \n",
    "        elif (inside_code >= 65281 and inside_code <= 65374): #全角字符（除空格）根据关系转化\n",
    "            inside_code -= 65248\n",
    "\n",
    "        rstring += chr(inside_code)\n",
    "    return rstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text,vocab):\n",
    "    text=strQ2B(text)\n",
    "    text=re.sub(r'\\ue40c','',text)\n",
    "    text=re.sub('[:「」￥…，,(【嘻嘻】)【哈哈】;\"”“+/—!. _ - % \\[\\]*◎《》、。]', '', text)  # 去特殊字符\n",
    "    text=re.sub('(www\\.(.*?)\\.com)|(http://(.*?)\\.com)','',text.lower()) #去URL\n",
    "    text=re.sub('[a-zA-Z]+',' EN ',text) # 去英文\n",
    "    text=re.sub('([\\d]*年*[\\d]*月*[\\d]+日+)|([\\d]+年+)|([\\d]*年*[\\d]+月+)',' DATE ',text)#去日期\n",
    "    text=re.sub('[\\d]+',' NUMBER ',text) #去数字\n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    text=' '.join(seg_list)\n",
    "    for word in text.split(' '):\n",
    "        vocab[word]=vocab.get(word,0)+1\n",
    "    #结巴分词\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_form(path,title_path,content_path,vocab):\n",
    "    contents=[]\n",
    "    titles=[]\n",
    "    with open(path,'r',encoding='ansi')as f:\n",
    "        text=''.join(f.readlines())\n",
    "        p=re.compile('<contenttitle>(.*)</contenttitle>\\n<content>(.*)</content>')\n",
    "        for temp in p.finditer(text):\n",
    "            title=(temp.group(1))\n",
    "            content=(temp.group(2))\n",
    "            if title.strip()!='' and content.strip()!='':\n",
    "                s1=clean(title,vocab)\n",
    "                if len(s1)>60 :\n",
    "                    s1=s1[:60]\n",
    "                s2=clean(content,vocab)\n",
    "                if len(s2)>240 :\n",
    "                    s2=s2[:240]\n",
    "                titles.append(s1)\n",
    "                contents.append(s2)\n",
    "    with open(title_path,'w',encoding='utf-8') as wf:\n",
    "        for temp in titles:\n",
    "            wf.write(temp+'\\n')\n",
    "    with open(content_path,'w',encoding='utf-8') as wf:\n",
    "        for temp in contents:\n",
    "            wf.write(temp+'\\n')\n",
    "    return contents,titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path=\"./data_test\"\n",
    "form_path='./train_test'\n",
    "vocab_path='./vocab/vocab.txt'\n",
    "max_vocabulary_size=50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_raw_fom(raw_path,form_path,vocab_path,max_vocabulary_size):\n",
    "    vocab={}\n",
    "    special_words = ['<PAD>', '<UNK>', '<GO>',  '<EOS>']\n",
    "    file_list=os.listdir(raw_path)\n",
    "    print(file_list)\n",
    "    for item in file_list:\n",
    "        _,_=raw_form(os.path.join(raw_path,item),os.path.join(form_path,'title_'+item),os.path.join(form_path,'content_'+item),vocab)\n",
    "    \n",
    "    vocab_list=special_words+sorted(vocab, key=vocab.get, reverse=True)\n",
    "    if len(vocab_list)>max_vocabulary_size:\n",
    "        vocab_list=vocab_list[:max_vocabulary_size]\n",
    "    with open(vocab_path,'w',encoding='utf-8')as f:\n",
    "        for w in vocab_list:\n",
    "            f.write(w+'\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['news1.txt', 'news2.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\jiangpin\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 3.124 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "all_raw_fom(raw_path,form_path,vocab_path,max_vocabulary_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入字典 生成字典 输出tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fom_tokenize(form_path,tokenize_path,vocab_path):\n",
    "\n",
    "    vocab_list=[]\n",
    "    with open(vocab_path,'r',encoding='utf-8')as f:\n",
    "        for item in f.readlines():\n",
    "            vocab_list.append(item.strip())\n",
    "    int_to_vocab = {idx: word for idx, word in enumerate(vocab_list)}\n",
    "    vocab_to_int = {word: idx for idx, word in int_to_vocab.items()}\n",
    "    file_list=os.listdir(form_path)\n",
    "    print(file_list)\n",
    "    for item in file_list:\n",
    "        with open(os.path.join(form_path,item),'r',encoding='utf-8')as f:\n",
    "#             tokenize=[]\n",
    "#             for sentence in f.readlines():\n",
    "#                 sentence_list=sentence.strip().split(' ')\n",
    "#                 temp=[]\n",
    "#                 for word in sentence_list:\n",
    "#                     temp.append()\n",
    "            if re.compile('title').search(item):\n",
    "                tokenize=[[vocab_to_int.get(word,vocab_to_int['<UNK>']) for word in sentence.strip().split(' ')]+[vocab_to_int['<EOS>']]for sentence in f.readlines()]\n",
    "            else:\n",
    "                tokenize=[[vocab_to_int.get(word,vocab_to_int['<UNK>']) for word in sentence.strip().split(' ')]for sentence in f.readlines()]\n",
    "            with open(os.path.join(tokenize_path,item),'w',encoding='utf-8')as wf:\n",
    "                for line in tokenize:\n",
    "                    s=' '.join([str(w) for w in line])\n",
    "                    wf.write(s+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_path='./train_test'\n",
    "tokenize_path='./tokenize'\n",
    "vocab_path='./vocab/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['content_news1.txt', 'content_news2.txt', 'title_news1.txt', 'title_news2.txt']\n"
     ]
    }
   ],
   "source": [
    "fom_tokenize(form_path,tokenize_path,vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path='./vocab/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(vocab_path):\n",
    "    vocab_list=[]\n",
    "    with open(vocab_path,'r',encoding='utf-8')as f:\n",
    "        for item in f.readlines():\n",
    "            vocab_list.append(item.strip())\n",
    "    int_to_vocab = {idx: word for idx, word in enumerate(vocab_list)}\n",
    "    vocab_to_int = {word: idx for idx, word in int_to_vocab.items()}\n",
    "    return int_to_vocab,vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_int_to_letter, source_letter_to_int=get_vocab(vocab_path)\n",
    "target_int_to_letter, target_letter_to_int =source_int_to_letter, source_letter_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_int_to_letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    '''\n",
    "    模型输入tensor\n",
    "    '''\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    # 定义target序列最大长度（之后target_sequence_length和source_sequence_length会作为feed_dict的参数）\n",
    "    target_sequence_length = tf.placeholder(tf.int32, (None,), name='target_sequence_length')\n",
    "    max_target_sequence_length = tf.reduce_max(target_sequence_length, name='max_target_len')\n",
    "    source_sequence_length = tf.placeholder(tf.int32, (None,), name='source_sequence_length')\n",
    "    \n",
    "    return inputs, targets, learning_rate, target_sequence_length, max_target_sequence_length, source_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_layer(input_data, rnn_size, num_layers,\n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "\n",
    "    '''\n",
    "    构造Encoder层\n",
    "    \n",
    "    参数说明：\n",
    "    - input_data: 输入tensor\n",
    "    - rnn_size: rnn隐层结点数量\n",
    "    - num_layers: 堆叠的rnn cell数量\n",
    "    - source_sequence_length: 源数据的序列长度\n",
    "    - source_vocab_size: 源数据的词典大小\n",
    "    - encoding_embedding_size: embedding的大小\n",
    "    '''\n",
    "    # Encoder embedding\n",
    "    encoder_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, encoding_embedding_size)\n",
    "\n",
    "    # RNN cell\n",
    "    def get_lstm_cell(rnn_size):\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return lstm_cell\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    encoder_output, encoder_state = tf.nn.dynamic_rnn(cell, encoder_embed_input, \n",
    "                                                      sequence_length=source_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    return encoder_output, encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decoder_input(data, vocab_to_int, batch_size):\n",
    "    '''\n",
    "    补充<GO>，并移除最后一个字符\n",
    "    \n",
    "    这里是为了构建 Decoder训练时的输入数据，使用target 而不是预测出的数据，提高精度\n",
    "    '''\n",
    "    # cut掉最后一个字符\n",
    "    ending = tf.strided_slice(data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    decoder_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(target_letter_to_int, decoding_embedding_size, num_layers, rnn_size,\n",
    "                   target_sequence_length, max_target_sequence_length, encoder_state, decoder_input,\n",
    "                   encoder_outputs,source_sequence_length):\n",
    "    '''\n",
    "    构造Decoder层\n",
    "    \n",
    "    参数：\n",
    "    - target_letter_to_int: target数据的映射表\n",
    "    - decoding_embedding_size: embed向量大小\n",
    "    - num_layers: 堆叠的RNN单元数量\n",
    "    - rnn_size: RNN单元的隐层结点数量\n",
    "    - target_sequence_length: target数据序列长度\n",
    "    - max_target_sequence_length: target数据序列最大长度\n",
    "    - encoder_state: encoder端编码的状态向量\n",
    "    - decoder_input: decoder端输入\n",
    "    \n",
    "    - encoder_outputs :添加一个注意力机制\n",
    "    - source_sequence_length 源数据长度\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # attention_states: [batch_size, max_time, num_units]\n",
    "    attention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "\n",
    "    # Create an attention mechanism\n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        num_units, attention_states,\n",
    "        memory_sequence_length=source_sequence_length)\n",
    "    \n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "    decoder_cell, attention_mechanism,\n",
    "    attention_layer_size=num_units)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # 1. Embedding\n",
    "    target_vocab_size = len(target_letter_to_int)\n",
    "    decoder_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    decoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings, decoder_input)\n",
    "\n",
    "    # 2. 构造Decoder中的RNN单元\n",
    "    def get_decoder_cell(rnn_size):\n",
    "        decoder_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return decoder_cell\n",
    "    \n",
    "    #2.1 添加注意力机制的RNN 单元\n",
    "    def get_decoder_cell_attention(rnn_size):\n",
    "         # attention_states: [batch_size, max_time, num_units]\n",
    "#        attention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "        attention_states=encoder_outputs\n",
    "         # Create an attention mechanism\n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        rnn_size, attention_states,\n",
    "        memory_sequence_length=source_sequence_length)\n",
    "        \n",
    "        decoder_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                            decoder_cell, attention_mechanism,\n",
    "                            attention_layer_size=rnn_size)\n",
    "        \n",
    "        return decoder_cell\n",
    "    \n",
    "    \n",
    "#     cell = tf.contrib.rnn.MultiRNNCell([get_decoder_cell(rnn_size) for _ in range(num_layers)])\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([get_decoder_cell_attention(rnn_size) for _ in range(num_layers)])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 3. Output全连接层\n",
    "    output_layer = Dense(target_vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "\n",
    "\n",
    "    # 4. Training decoder\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        # 得到help对象\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embed_input,\n",
    "                                                            sequence_length=target_sequence_length,\n",
    "                                                            time_major=False)\n",
    "        # 构造decoder\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n",
    "                                                           training_helper,\n",
    "                                                           initial_state=cell.zero_state(dtype=tf.float32,batch_size=batch_size)\n",
    "                                                        ,output_layer=output_layer) \n",
    "\n",
    "        training_decoder_output, _ ,_= tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                                       impute_finished=True,\n",
    "                                                                       maximum_iterations=max_target_sequence_length)\n",
    "    # 5. Predicting decoder\n",
    "    # 与training共享参数\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        # 创建一个常量tensor并复制为batch_size的大小\n",
    "        start_tokens = tf.tile(tf.constant([target_letter_to_int['<GO>']], dtype=tf.int32), [batch_size], \n",
    "                               name='start_tokens')\n",
    "        predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                target_letter_to_int['<EOS>'])\n",
    "        predicting_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n",
    "                                                        predicting_helper,\n",
    "                                                        initial_state=cell.zero_state(dtype=tf.float32,batch_size=batch_size)\n",
    "                                                            ,output_layer=output_layer)\n",
    "        predicting_decoder_output, _,_  = tf.contrib.seq2seq.dynamic_decode(predicting_decoder,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_target_sequence_length)\n",
    "    \n",
    "    return training_decoder_output, predicting_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, targets, lr, target_sequence_length, \n",
    "                  max_target_sequence_length, source_sequence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  encoder_embedding_size, decoder_embedding_size, \n",
    "                  rnn_size, num_layers):\n",
    "    \n",
    "    # 获取encoder的状态输出\n",
    "    encoder_outputs, encoder_state = get_encoder_layer(input_data, \n",
    "                                  rnn_size, \n",
    "                                  num_layers, \n",
    "                                  source_sequence_length,\n",
    "                                  source_vocab_size, \n",
    "                                  encoding_embedding_size)\n",
    "    \n",
    "    \n",
    "    # 预处理后的decoder输入\n",
    "    decoder_input = process_decoder_input(targets, target_letter_to_int, batch_size)\n",
    "    \n",
    "    # 将状态向量与输入传递给decoder\n",
    "    training_decoder_output, predicting_decoder_output = decoding_layer(target_letter_to_int, \n",
    "                                                                       decoding_embedding_size, \n",
    "                                                                       num_layers, \n",
    "                                                                       rnn_size,\n",
    "                                                                       target_sequence_length,\n",
    "                                                                       max_target_sequence_length,\n",
    "                                                                       encoder_state, \n",
    "                                                                       decoder_input,\n",
    "                                                                        encoder_outputs,\n",
    "                                                                        source_sequence_length\n",
    "                                                                       ) \n",
    "    \n",
    "    return training_decoder_output, predicting_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "# Number of Epochs\n",
    "epochs =5\n",
    "# Batch Size\n",
    "batch_size =32\n",
    "# RNN Size\n",
    "rnn_size = 50\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 128\n",
    "decoding_embedding_size = 128\n",
    "# Learning Rate\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造graph\n",
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # 获得模型输入    \n",
    "    input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length = get_inputs()\n",
    "    \n",
    "    training_decoder_output, predicting_decoder_output = seq2seq_model(input_data, \n",
    "                                                                      targets, \n",
    "                                                                      lr, \n",
    "                                                                      target_sequence_length, \n",
    "                                                                      max_target_sequence_length, \n",
    "                                                                      source_sequence_length,\n",
    "                                                                      len(source_letter_to_int),\n",
    "                                                                      len(target_letter_to_int),\n",
    "                                                                      encoding_embedding_size, \n",
    "                                                                      decoding_embedding_size, \n",
    "                                                                      rnn_size, \n",
    "                                                                      num_layers)    \n",
    "    \n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output, 'logits')\n",
    "    predicting_logits = tf.identity(predicting_decoder_output.sample_id, name='predictions')\n",
    "    \n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        \n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicDecoderOutput(rnn_output=<tf.Tensor 'decode/decoder/transpose:0' shape=(32, ?, 50000) dtype=float32>, sample_id=<tf.Tensor 'decode/decoder/transpose_1:0' shape=(32, ?) dtype=int32>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    '''\n",
    "    对batch中的序列进行补全，保证batch中的每行都有相同的sequence_length\n",
    "    \n",
    "    参数：\n",
    "    - sentence batch\n",
    "    - pad_int: <PAD>对应索引号\n",
    "    '''\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(file_list,tokenize_path,batch_size,pad_int):\n",
    "    '''\n",
    "    定义生成器，用来获取tokenize下的所有content\n",
    "    '''\n",
    "    for item in file_list:\n",
    "        source_path=os.path.join(tokenize_path,'content_'+item)\n",
    "        target_path=os.path.join(tokenize_path,'title_'+item)\n",
    "        with open(source_path,'r',encoding='utf-8')as sf:\n",
    "            sources=[[int(word) for word in sentence.strip().split(' ')]for sentence in sf.readlines()]\n",
    "        with open(target_path,'r',encoding='utf-8')as tf:\n",
    "            targets=[[int(word) for word in sentence.strip().split(' ')]for sentence in tf.readlines()]\n",
    "        \n",
    "        for batch_i in range(0, len(sources)//batch_size):\n",
    "            start_i = batch_i * batch_size\n",
    "            sources_batch = sources[start_i:start_i + batch_size]\n",
    "            targets_batch = targets[start_i:start_i + batch_size]\n",
    "            # 补全序列\n",
    "            pad_sources_batch = np.array(pad_sentence_batch(sources_batch, pad_int))\n",
    "            pad_targets_batch = np.array(pad_sentence_batch(targets_batch, pad_int))\n",
    "\n",
    "            # 记录每条记录的长度\n",
    "            targets_lengths = []\n",
    "            for target in targets_batch:\n",
    "                targets_lengths.append(len(target))\n",
    "\n",
    "            source_lengths = []\n",
    "            for source in sources_batch:\n",
    "                source_lengths.append(len(source))\n",
    "\n",
    "            yield pad_targets_batch, pad_sources_batch, targets_lengths, source_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='./data_test'\n",
    "file_list=os.listdir(data_path)\n",
    "tokenize_path='./tokenize'\n",
    "batch_size=32\n",
    "pad_int=source_letter_to_int['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "(valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) = next(get_batches(file_list,tokenize_path,batch_size,pad_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/5 Batch    0/未知 - Training Loss: 10.812  - Validation loss:  8.279\n",
      "Epoch   1/5 Batch   50/未知 - Training Loss:  7.491  - Validation loss:  7.541\n",
      "Epoch   1/5 Batch  100/未知 - Training Loss:  7.634  - Validation loss:  7.458\n",
      "Epoch   1/5 Batch  150/未知 - Training Loss:  7.276  - Validation loss:  6.834\n",
      "Epoch   1/5 Batch  200/未知 - Training Loss:  7.299  - Validation loss:  6.952\n",
      "Epoch   2/5 Batch    0/未知 - Training Loss:  6.952  - Validation loss:  6.973\n",
      "Epoch   2/5 Batch   50/未知 - Training Loss:  6.195  - Validation loss:  7.564\n",
      "Epoch   2/5 Batch  100/未知 - Training Loss:  7.556  - Validation loss:  8.138\n",
      "Epoch   2/5 Batch  150/未知 - Training Loss:  6.772  - Validation loss:  7.762\n",
      "Epoch   2/5 Batch  200/未知 - Training Loss:  6.962  - Validation loss:  7.319\n",
      "Epoch   3/5 Batch    0/未知 - Training Loss:  7.319  - Validation loss:  7.204\n",
      "Epoch   3/5 Batch   50/未知 - Training Loss:  6.544  - Validation loss:  7.876\n",
      "Epoch   3/5 Batch  100/未知 - Training Loss:  8.087  - Validation loss:  8.969\n",
      "Epoch   3/5 Batch  150/未知 - Training Loss:  6.830  - Validation loss:  7.424\n",
      "Epoch   3/5 Batch  200/未知 - Training Loss:  7.567  - Validation loss:  7.017\n",
      "Epoch   4/5 Batch    0/未知 - Training Loss:  7.017  - Validation loss:  6.606\n",
      "Epoch   4/5 Batch   50/未知 - Training Loss:  6.019  - Validation loss:  7.298\n",
      "Epoch   4/5 Batch  100/未知 - Training Loss:  6.559  - Validation loss:  7.703\n",
      "Epoch   4/5 Batch  150/未知 - Training Loss:  6.578  - Validation loss:  8.264\n",
      "Epoch   4/5 Batch  200/未知 - Training Loss:  6.438  - Validation loss:  6.822\n",
      "Epoch   5/5 Batch    0/未知 - Training Loss:  6.822  - Validation loss:  6.500\n",
      "Epoch   5/5 Batch   50/未知 - Training Loss:  5.611  - Validation loss:  7.679\n",
      "Epoch   5/5 Batch  100/未知 - Training Loss:  7.172  - Validation loss:  8.480\n",
      "Epoch   5/5 Batch  150/未知 - Training Loss:  6.038  - Validation loss:  7.696\n",
      "Epoch   5/5 Batch  200/未知 - Training Loss:  6.124  - Validation loss:  6.520\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "display_step = 50 # 每隔50轮输出loss\n",
    "\n",
    "# 查看是否有 checkpoint_f 这个文件，无则新建一个\n",
    "if not os.path.exists('./checkpoint_f'):\n",
    "        os.makedirs('./checkpoint_f')\n",
    "\n",
    "checkpoint = \"./checkpoint_f/trained_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        for batch_i, (targets_batch, sources_batch, targets_lengths, sources_lengths) in enumerate(\n",
    "                get_batches(file_list,tokenize_path,batch_size,pad_int)):\n",
    "            \n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: sources_batch,\n",
    "                 targets: targets_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 source_sequence_length: sources_lengths})\n",
    "\n",
    "            if batch_i % display_step == 0:\n",
    "            \n",
    "                # 计算validation loss\n",
    "                validation_loss = sess.run(\n",
    "                [cost],\n",
    "                {input_data: valid_sources_batch,\n",
    "                 targets: valid_targets_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: valid_targets_lengths,\n",
    "                 source_sequence_length: valid_sources_lengths})\n",
    "                \n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Training Loss: {:>6.3f}  - Validation loss: {:>6.3f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              '未知', \n",
    "                              loss, \n",
    "                              validation_loss[0]))\n",
    "\n",
    "    \n",
    "    \n",
    "    # 保存模型\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, checkpoint)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_to_seq(text):\n",
    "    '''\n",
    "    对源数据进行转换\n",
    "    '''\n",
    "    sequence_length = 120\n",
    "    return [source_letter_to_int.get(word, source_letter_to_int['<UNK>']) for word in text.split(' ')] + [source_letter_to_int['<PAD>']]*(sequence_length-len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoint_f/trained_model.ckpt\n",
      "原始输入: \n",
      " 我 来说 两句 广州 男人 爱 拼 才会赢 无可否认 广州 男人 发了 广州 男人 财大气粗 似乎 分分秒秒 都 在 挣钱 广州 男人 形象 并 不 高大 皮肤 也 黝黑 但 他们 用 豪华 私家车 别墅 名牌 西服 名牌 衬衣 名牌 皮鞋 所 包装 出 的 气派 却是 令 许多 女人 心仪 不已 的 无牌 不 穿 无牌 不用 玩 的 就是 人民币 这 还 不 容易 吗 ? 广州 男人 有 一点点 坏 可话 又 说 回来 男人 不 坏 女人 不爱 嘛 而 被 天南地北 蜂涌\n",
      " \n",
      "\n",
      "Source\n",
      "  Word 编号:    [1, 17, 168, 558, 593, 880, 960, 6032, 1, 47153, 593, 880, 15775, 593, 880, 28624, 1040, 47154, 22, 7, 11648, 593, 880, 1120, 49, 24, 6544, 1852, 13, 25494, 35, 51, 157, 2323, 4347, 760, 5522, 21054, 5522, 19374, 5522, 14891, 159, 4065, 201, 5, 17978, 2124, 1187, 526, 915, 16806, 6945, 5, 19375, 24, 1192, 19375, 1982, 2815, 5, 83, 430, 34, 44, 24, 825, 454, 39, 593, 880, 14, 8056, 7124, 1, 93, 33, 2248, 880, 24, 7124, 915, 21055, 6545, 36, 47, 1, 1, 4]\n",
      "  Input Words: <UNK> 我 来说 两句 广州 男人 爱 拼 <UNK> 无可否认 广州 男人 发了 广州 男人 财大气粗 似乎 分分秒秒 都 在 挣钱 广州 男人 形象 并 不 高大 皮肤 也 黝黑 但 他们 用 豪华 私家车 别墅 名牌 西服 名牌 衬衣 名牌 皮鞋 所 包装 出 的 气派 却是 令 许多 女人 心仪 不已 的 无牌 不 穿 无牌 不用 玩 的 就是 人民币 这 还 不 容易 吗 ? 广州 男人 有 一点点 坏 <UNK> 又 说 回来 男人 不 坏 女人 不爱 嘛 而 被 <UNK> <UNK> \n",
      "\n",
      "Target\n",
      "  Word 编号:       [1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "  Response Words: <UNK>                                                                                                                                                                                                                                                  \n"
     ]
    }
   ],
   "source": [
    "# 输入一个单词\n",
    "input_word = '''\n",
    " 我 来说 两句 广州 男人 爱 拼 才会赢 无可否认 广州 男人 发了 广州 男人 财大气粗 似乎 分分秒秒 都 在 挣钱 广州 男人 形象 并 不 高大 皮肤 也 黝黑 但 他们 用 豪华 私家车 别墅 名牌 西服 名牌 衬衣 名牌 皮鞋 所 包装 出 的 气派 却是 令 许多 女人 心仪 不已 的 无牌 不 穿 无牌 不用 玩 的 就是 人民币 这 还 不 容易 吗 ? 广州 男人 有 一点点 坏 可话 又 说 回来 男人 不 坏 女人 不爱 嘛 而 被 天南地北 蜂涌\n",
    " '''\n",
    "text = source_to_seq(input_word)\n",
    "\n",
    "checkpoint = \"./checkpoint_f/trained_model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # 加载模型\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    \n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      target_sequence_length: [len(input_word)]*batch_size, \n",
    "                                      source_sequence_length: [len(input_word)]*batch_size})[0] \n",
    "\n",
    "\n",
    "pad = source_letter_to_int[\"<PAD>\"] \n",
    "\n",
    "print('原始输入:', input_word)\n",
    "\n",
    "print('\\nSource')\n",
    "print('  Word 编号:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join([source_int_to_letter[i] for i in text])))\n",
    "\n",
    "print('\\nTarget')\n",
    "print('  Word 编号:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join([target_int_to_letter[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
